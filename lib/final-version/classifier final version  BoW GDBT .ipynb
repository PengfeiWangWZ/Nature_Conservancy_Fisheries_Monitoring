{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yueyingteng/anaconda3/envs/python2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from cv2 import *\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import time\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('/Users/yueyingteng/Documents/2016.9/Big Data /kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image_paths\n",
    "folders = [f for f in os.listdir('./fish') if not f.startswith('.')]\n",
    "image_paths0 = []\n",
    "nums = []\n",
    "for folder in folders:\n",
    "    image_name = os.listdir(os.path.join('./fish',folder))\n",
    "    image_names = image_name[0:len(image_name)-1]\n",
    "    image_paths0.append([os.path.join(os.path.join('./fish',folder), f) for f in image_names])\n",
    "    nums.append((len(image_name)-1))\n",
    "image_paths = [item for sublist in image_paths0 for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yueyingteng/anaconda3/envs/python2/lib/python2.7/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((1, 2544), 0) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/Users/yueyingteng/anaconda3/envs/python2/lib/python2.7/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((1, 307), 1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/Users/yueyingteng/anaconda3/envs/python2/lib/python2.7/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((1, 123), 2) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/Users/yueyingteng/anaconda3/envs/python2/lib/python2.7/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((1, 100), 3) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/Users/yueyingteng/anaconda3/envs/python2/lib/python2.7/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((1, 388), 4) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/Users/yueyingteng/anaconda3/envs/python2/lib/python2.7/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((1, 195), 5) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/Users/yueyingteng/anaconda3/envs/python2/lib/python2.7/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full((1, 822), 6) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#image_classes\n",
    "image_classes = []\n",
    "for i in range(len(nums)):\n",
    "    image_classes.append(np.full((1,nums[i]),i))\n",
    "image_classes = np.concatenate(image_classes, axis = 1)\n",
    "\n",
    "# flatten out the list of lists in image_classes\n",
    "a = np.ravel(image_classes)\n",
    "image_classes = a.tolist()\n",
    "image_classes_train = image_classes[0:len(image_classes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of feature creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_det = xfeatures2d.SIFT_create()\n",
    "\n",
    "def preProcessImages(image_paths):\n",
    "    descriptors= []\n",
    "    for image_path in image_paths:\n",
    "        im = imread(image_path)\n",
    "        kpts = feature_det.detect(im)\n",
    "        # kpts, des = descr_ext.compute(im, kpts)\n",
    "        kpts, des = feature_det.compute(im, kpts)\n",
    "        descriptors.append(des)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre process all training image and prepare for the creation of image feature dictionary\n",
    "start = time.time()\n",
    "descriptors= preProcessImages(image_paths)\n",
    "end = time.time()\n",
    "print \"minutes spent in Descriptors\"\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove image paths and image classes that has empty descriptors after preprocessing \n",
    "descriptors_none=[]\n",
    "for i, j in enumerate(descriptors):\n",
    "    if j == None:\n",
    "        descriptors_none.append(i)\n",
    "\n",
    "descriptors=[i for i in descriptors if i!= None]\n",
    "image_classes_train=[image_classes_train[i] for i in range(len(image_classes_train)) if i not in descriptors_none]\n",
    "image_paths=[image_paths[i] for i in range(len(image_paths)) if i not in descriptors_none]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matcher = BFMatcher(NORM_L2)\n",
    "\n",
    "# extract descriptor of the new images baesed on the constructed vocabulary\n",
    "# bow_extract  =cv2.BOWImgDescriptorExtractor(descr_ext,matcher)\n",
    "bow_extract  = BOWImgDescriptorExtractor(feature_det,matcher)\n",
    "\n",
    "def getImagedata(feature_det,bow_extract,path):\n",
    "    im = imread(path)\n",
    "    featureset = bow_extract.compute(im, feature_det.detect(im))\n",
    "    return featureset\n",
    "# returned featureset contains normzlized histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clustering k=500\n",
    "bow_train = BOWKMeansTrainer(500)\n",
    "\n",
    "# create the vocabulary\n",
    "for des in descriptors:\n",
    "    bow_train.add(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "voc = bow_train.cluster()\n",
    "bow_extract.setVocabulary(voc)\n",
    "\n",
    "end = time.time()\n",
    "print \"minutes spent in creating Vocabulary\"\n",
    "print (end - start)/60  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preserve the vocabulary \n",
    "joblib.dump((voc), \"fullvoc.pkl\", compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get features from the training data based on the vocabulary & approximate nearest neighbour\n",
    "# features are used as the training data \n",
    "traindata = []  \n",
    "\n",
    "start = time.time()\n",
    "for path in image_paths:\n",
    "    featureset = getImagedata(feature_det,bow_extract,path)\n",
    "    traindata.append(featureset)\n",
    "\n",
    "end = time.time()\n",
    "print (\"minutes spent in Extracting vocabulary\")\n",
    "print ((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change 3d array traindata to 2d array\n",
    "traindata = np.array(traindata).reshape(len(np.array(traindata)), -1)\n",
    "print traindata.shape\n",
    "## (4477, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train GBDT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient boosting decision tree\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# n_estimators: The number of boosting stages to perform. \n",
    "# Gradient boosting is fairly robust to over-fitting so a large n_estimators usually results in better performance.\n",
    "# max_depth: maximum depth of the individual regression estimators. \n",
    "# The maximum depth limits the number of nodes in the tree. \n",
    "# Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=1600, max_depth=3)\n",
    "model = model.fit(traindata, np.array(image_classes_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on training data\n",
    "predict = model.predict(traindata)\n",
    "proba = model.predict_proba(traindata)\n",
    "\n",
    "# evaluation metrics\n",
    "print (metrics.confusion_matrix(np.array(image_classes_train), predict))\n",
    "print (metrics.classification_report(np.array(image_classes_train), predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GBDT testing accuracy \n",
    "start = time.time()\n",
    "\n",
    "# evaluate the model by splitting into train(0.8) and test sets(0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(traindata, np.array(image_classes_train), test_size=0.2, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict class labels for the test set\n",
    "predicted = model.predict(X_test)\n",
    "# class probabilities for the test set\n",
    "probs = model.predict_proba(X_test)\n",
    "# sum(predicted == image_classes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GDBT 5-fold cross-validation with 'accuracy' scoring\n",
    "start = time.time()\n",
    "X = traindata\n",
    "y = np.array(image_classes_train)\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv= 5)\n",
    "print scores\n",
    "print scores.mean()\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GBDT parameter tuning \n",
    "from sklearn.grid_search import GridSearchCV  \n",
    "\n",
    "# two types of parameter: those relevant to boosting and those about decision trees \n",
    "\n",
    "# default setting :\n",
    "# learning_rate=0.1 (shrinkage).\n",
    "# n_estimators=100 (number of trees).\n",
    "# max_depth=3.\n",
    "# min_samples_split=2.\n",
    "# min_samples_leaf=1.\n",
    "# subsample=1.0.\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "X = traindata\n",
    "y = np.array(image_classes_train)\n",
    "\n",
    "# take the default learning rate of 0.1 and check the optimum number of trees\n",
    "param_test1 = {'n_estimators':range(1000,2000,200)}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(), \n",
    "param_grid = param_test1, scoring='accuracy', n_jobs=4, iid=False, cv=5)\n",
    "gsearch1.fit(X, y)\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check the output   ## best {'n_estimators': 1600}\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## best params: {'max_depth': 3}\n",
    "# parameter tuning for decision trees in the algorithm based on the best parameter selected in the previous step\n",
    "param_test2 = {'max_depth':range(3,9,2)}\n",
    "gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(n_estimators = 1600), \n",
    "param_grid = param_test2, scoring='accuracy', n_jobs=4, iid=False, cv=5)\n",
    "gsearch2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check result \n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best classifier according to tuning result \n",
    "start = time.time()\n",
    "best = GradientBoostingClassifier(n_estimators = 1600, max_depth = 3)\n",
    "best = best.fit(traindata, np.array(image_classes_train))\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best classifier CV\n",
    "start = time.time()\n",
    "X = traindata\n",
    "y = np.array(image_classes_train)\n",
    "\n",
    "scores = cross_val_score(best, X, y, scoring='accuracy', cv= 5)\n",
    "print scores\n",
    "print scores.mean()\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preserve the best GBDT model\n",
    "joblib.dump((best), \"GBDT.pkl\", compress=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [python2]",
   "language": "python",
   "name": "Python [python2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
