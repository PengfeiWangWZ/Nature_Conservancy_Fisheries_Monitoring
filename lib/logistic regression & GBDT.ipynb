{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yueyingteng/anaconda3/envs/python2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from cv2 import *\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import time\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('/Users/yueyingteng/Documents/2016.9/Big Data /kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image_paths\n",
    "folders = [f for f in os.listdir('./fish') if not f.startswith('.')]\n",
    "image_paths0 = []\n",
    "nums = []\n",
    "for folder in folders:\n",
    "    image_name = os.listdir(os.path.join('./fish',folder))\n",
    "    image_names = image_name[0:len(image_name)-1]\n",
    "    image_paths0.append([os.path.join(os.path.join('./fish',folder), f) for f in image_names])\n",
    "    nums.append((len(image_name)-1))\n",
    "image_paths = [item for sublist in image_paths0 for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#image_classes\n",
    "image_classes = []\n",
    "for i in range(len(nums)):\n",
    "    image_classes.append(np.full((1,nums[i]),i))\n",
    "image_classes = np.concatenate(image_classes, axis = 1)\n",
    "\n",
    "# flatten out the list of lists in image_classes\n",
    "a = np.ravel(image_classes)\n",
    "image_classes = a.tolist()\n",
    "image_classes_train = image_classes[0:len(image_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4091, 4091)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_paths), len(image_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_det = xfeatures2d.SIFT_create()\n",
    "\n",
    "def preProcessImages(image_paths):\n",
    "    descriptors= []\n",
    "    for image_path in image_paths:\n",
    "        im = imread(image_path)\n",
    "        kpts = feature_det.detect(im)\n",
    "        # kpts, des = descr_ext.compute(im, kpts)\n",
    "        kpts, des = feature_det.compute(im, kpts)\n",
    "        descriptors.append(des)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minutes spent in Descriptors\n",
      "2.48541778326\n"
     ]
    }
   ],
   "source": [
    "# pre process all training image and prepare for the creation of image feature dictionary\n",
    "start = time.time()\n",
    "descriptors= preProcessImages(image_paths)\n",
    "end = time.time()\n",
    "print \"minutes spent in Descriptors\"\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yueyingteng/anaconda3/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:4: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "/Users/yueyingteng/anaconda3/envs/python2/lib/python2.7/site-packages/ipykernel/__main__.py:7: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "# remove image paths and image classes that has empty descriptors after preprocessing \n",
    "descriptors_none=[]\n",
    "for i, j in enumerate(descriptors):\n",
    "    if j == None:\n",
    "        descriptors_none.append(i)\n",
    "\n",
    "descriptors=[i for i in descriptors if i!= None]\n",
    "image_classes_train=[image_classes_train[i] for i in range(len(image_classes_train)) if i not in descriptors_none]\n",
    "image_paths=[image_paths[i] for i in range(len(image_paths)) if i not in descriptors_none]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4089, 4089, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_classes_train), len(image_paths), len(descriptors_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matcher = BFMatcher(NORM_L2)\n",
    "\n",
    "# extract descriptor of the new images baesed on the constructed vocabulary\n",
    "# bow_extract  =cv2.BOWImgDescriptorExtractor(descr_ext,matcher)\n",
    "bow_extract  = BOWImgDescriptorExtractor(feature_det,matcher)\n",
    "\n",
    "def getImagedata(feature_det,bow_extract,path):\n",
    "    im = imread(path)\n",
    "    featureset = bow_extract.compute(im, feature_det.detect(im))\n",
    "    return featureset\n",
    "# returned featureset contains normzlized histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clustering k=500\n",
    "bow_train = BOWKMeansTrainer(500)\n",
    "\n",
    "# create the vocabulary\n",
    "for des in descriptors:\n",
    "    bow_train.add(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minutes spent in creating Vocabulary\n",
      "117.007270916\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "voc = bow_train.cluster()\n",
    "bow_extract.setVocabulary(voc)\n",
    "\n",
    "end = time.time()\n",
    "print \"minutes spent in creating Vocabulary\"\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fullvoc.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preserve the vocabulary \n",
    "joblib.dump((voc), \"fullvoc.pkl\", compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 128)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = joblib.load(\"fullvoc.pkl\")\n",
    "voc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minutes spent in Extracting vocabulary\n",
      "2.84950116873\n"
     ]
    }
   ],
   "source": [
    "# get features from the training data based on the vocabulary & approximate nearest neighbour\n",
    "# features are used as the training data \n",
    "traindata = []  \n",
    "\n",
    "start = time.time()\n",
    "for path in image_paths:\n",
    "    featureset = getImagedata(feature_det,bow_extract,path)\n",
    "    traindata.append(featureset)\n",
    "\n",
    "end = time.time()\n",
    "print (\"minutes spent in Extracting vocabulary\")\n",
    "print ((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4089, 500)\n"
     ]
    }
   ],
   "source": [
    "# change 3d array traindata to 2d array\n",
    "\n",
    "traindata = np.array(traindata).reshape(len(np.array(traindata)), -1)\n",
    "print traindata.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00533483425776\n"
     ]
    }
   ],
   "source": [
    "# logistic regression in multiclass classification \n",
    "#### how to decide the right class_weight\n",
    "#### difference between class_weight and smaple_weight \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "model = LogisticRegression(multi_class='ovr',class_weight='balanced')\n",
    "model = model.fit(traindata, np.array(image_classes_train))\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)/60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.30809356  0.15023497  0.10552539  0.13604493  0.13412517  0.16597597]\n",
      " [ 0.29749655  0.13320986  0.12070071  0.15618289  0.13309529  0.1593147 ]\n",
      " [ 0.23468415  0.13455724  0.19130948  0.12878013  0.10525634  0.20541267]\n",
      " ..., \n",
      " [ 0.2887445   0.14350396  0.1334561   0.13985877  0.13306079  0.16137588]\n",
      " [ 0.29136074  0.14759037  0.13695339  0.14385566  0.12082122  0.15941862]\n",
      " [ 0.28650706  0.14849101  0.13498216  0.14526417  0.12749622  0.15725939]]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "prob = model.predict_proba(traindata)\n",
    "pred = model.predict(traindata)\n",
    "print prob\n",
    "print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2538    0    1    0    2    2]\n",
      " [ 295    0    5    0    1    5]\n",
      " [  91    0   31    0    0    1]\n",
      " [ 100    0    0    0    0    0]\n",
      " [ 177    0    0    0   18    0]\n",
      " [ 756    0   28    0    0   38]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.64      1.00      0.78      2543\n",
      "        1.0       0.00      0.00      0.00       306\n",
      "        2.0       0.48      0.25      0.33       123\n",
      "        3.0       0.00      0.00      0.00       100\n",
      "        4.0       0.86      0.09      0.17       195\n",
      "        5.0       0.83      0.05      0.09       822\n",
      "\n",
      "avg / total       0.62      0.64      0.52      4089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (metrics.confusion_matrix(np.array(image_classes_train), pred))\n",
    "print (metrics.classification_report(np.array(image_classes_train), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64196625091709469"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(traindata, np.array(image_classes_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  4.,  2.,  5.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pd.DataFrame(pred, columns = ['classes'])\n",
    "pred['classes'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classes</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>2543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         paths\n",
       "classes       \n",
       "0.0       2543\n",
       "1.0        306\n",
       "2.0        123\n",
       "3.0        100\n",
       "4.0        195\n",
       "5.0        822"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class 1, 3 are missing \n",
    "label = pd.read_csv('fishes.csv')\n",
    "label.groupby('classes').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gradient boosting decision tree\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# n_estimators: The number of boosting stages to perform. \n",
    "# Gradient boosting is fairly robust to over-fitting so a large n_estimators usually results in better performance.\n",
    "# max_depth: maximum depth of the individual regression estimators. \n",
    "# The maximum depth limits the number of nodes in the tree. \n",
    "# Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=200, max_depth=3)\n",
    "model = model.fit(traindata, np.array(image_classes_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3878"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = model.predict(traindata)\n",
    "proba = model.predict_proba(traindata)\n",
    "sum(predict == image_classes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2541    0    0    0    0    2]\n",
      " [   6  300    0    0    0    0]\n",
      " [   1    0  122    0    0    0]\n",
      " [   0    0    0  100    0    0]\n",
      " [   1    0    0    0  194    0]\n",
      " [  10    0    0    0    0  812]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      1.00      1.00      2543\n",
      "        1.0       1.00      0.98      0.99       306\n",
      "        2.0       1.00      0.99      1.00       123\n",
      "        3.0       1.00      1.00      1.00       100\n",
      "        4.0       1.00      0.99      1.00       195\n",
      "        5.0       1.00      0.99      0.99       822\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (metrics.confusion_matrix(np.array(image_classes_train), predict))\n",
    "print (metrics.classification_report(np.array(image_classes_train), predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99510882856444116"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(traindata, np.array(image_classes_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.82190585136\n"
     ]
    }
   ],
   "source": [
    "# GDBT testing accuracy \n",
    "start = time.time()\n",
    "\n",
    "# evaluate the model by splitting into train(0.8) and test sets(0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(traindata, np.array(image_classes_train), test_size=0.2, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict class labels for the test set\n",
    "predicted = model.predict(X_test)\n",
    "# class probabilities for the test set\n",
    "probs = model.predict_proba(X_test)\n",
    "# sum(predicted == image_classes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74816625916870416"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[477   1   2   0   1  12]\n",
      " [ 30  16   0   0   0   6]\n",
      " [  7   0   7   0   0   7]\n",
      " [ 16   0   0   9   0   1]\n",
      " [ 25   0   0   0  13   7]\n",
      " [ 90   1   0   0   0  90]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.74      0.97      0.84       493\n",
      "        1.0       0.89      0.31      0.46        52\n",
      "        2.0       0.78      0.33      0.47        21\n",
      "        3.0       1.00      0.35      0.51        26\n",
      "        4.0       0.93      0.29      0.44        45\n",
      "        5.0       0.73      0.50      0.59       181\n",
      "\n",
      "avg / total       0.77      0.75      0.72       818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to see the confusion matrix and a classification report with other metrics.\n",
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print (metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.75487805  0.78510379  0.7799511   0.76102941  0.71813725]\n",
      "0.759819920159\n"
     ]
    }
   ],
   "source": [
    "# GDBT 5-fold cross-validation with 'accuracy' scoring\n",
    "start = time.time()\n",
    "X = traindata\n",
    "y = np.array(image_classes_train)\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv= 5)\n",
    "print scores\n",
    "print scores.mean()\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GBDT parameter tuning \n",
    "from sklearn.grid_search import GridSearchCV  \n",
    "\n",
    "# two types of parameter: those relevant to boosting and those about decision trees \n",
    "\n",
    "# default setting :\n",
    "# learning_rate=0.1 (shrinkage).\n",
    "# n_estimators=100 (number of trees).\n",
    "# max_depth=3.\n",
    "# min_samples_split=2.\n",
    "# min_samples_leaf=1.\n",
    "# subsample=1.0.\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "X = traindata\n",
    "y = np.array(image_classes_train)\n",
    "\n",
    "# take the default learning rate of 0.1 and check the optimum number of trees\n",
    "param_test1 = {'n_estimators':range(300,1000,100)}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(), \n",
    "param_grid = param_test1, scoring='accuracy', n_jobs=4, iid=False, cv=5)\n",
    "gsearch1.fit(X, y)\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check the output\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameter tuning for decision trees in the algorithm based on the best parameter selected in the previous step\n",
    "param_test2 = {'max_depth':range(3,9,2)}\n",
    "gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier( n_estimators = gsearch1.best_params_), \n",
    "param_grid = param_test2, scoring='accuracy', n_jobs=4, iid=False, cv=5)\n",
    "gsearch2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check result \n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best classifier according to tuning result \n",
    "start = time.time()\n",
    "best = GradientBoostingClassifier(n_estimators = gsearch1.best_params_, max_depth = gsearch2.best_params_)\n",
    "best = best.fit(traindata, np.array(image_classes_train))\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best classifier CV\n",
    "start = time.time()\n",
    "X = traindata\n",
    "y = np.array(image_classes_train)\n",
    "\n",
    "scores = cross_val_score(best, X, y, scoring='accuracy', cv= 5)\n",
    "print scores\n",
    "print scores.mean()\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the best model by splitting into train(0.8) and test sets(0.2)\n",
    "start = time.time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(traindata, np.array(image_classes_train), test_size=0.2, random_state=0)\n",
    "best.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print (end - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict class labels \n",
    "predict_best = best.predict(X_test)\n",
    "# class probabilities \n",
    "probs_best = best.predict_proba(X_test)\n",
    "# sum(predicted == image_classes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# confusion matrix and a classification report with other metrics \n",
    "print (metrics.confusion_matrix(y_test, predict_best))\n",
    "print (metrics.classification_report(y_test, predicted_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03289226,  0.05975397,  0.03150069, ...,  0.02272831,\n",
       "         0.0266453 ,  0.08829331]), 0.032892264752132444)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probbb= model.predict_proba(traindata)\n",
    "probbb[:,1],probbb[:,1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00647770854854\n",
      "6.37451100843e-05\n",
      "2.28096609254e-05\n",
      "0.992622341031\n",
      "0.000123997831948\n",
      "0.000689397817067\n",
      "(4089, 6)\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print probbb[:,i][3000]\n",
    "    \n",
    "print probbb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predd = model.predict(traindata)\n",
    "predd[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python2]",
   "language": "python",
   "name": "Python [python2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
